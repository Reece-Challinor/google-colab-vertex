{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'sklearn'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import uuid\n",
        "import logging\n",
        "from typing import List, Dict, Union, Optional, Tuple, Any\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "from google.cloud import aiplatform\n",
        "from google.cloud.aiplatform.gapic.schema import predict\n",
        "import vertexai\n",
        "from vertexai.language_models import TextGenerationModel\n",
        "from vertexai.preview.generative_models import GenerativeModel\n",
        "from langchain_google_vertexai import VertexAIEmbeddings, ChatVertexAI\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "\n",
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# # Google Vertex AI LLM Data Processing Pipeline\n",
        "# \n",
        "# A comprehensive demo showcasing how to build a data processing pipeline\n",
        "# using Google Vertex AI and Llama 3.3 LLM.\n",
        "\n",
        "# ## 1. Setup and Installation\n",
        "\n",
        "# Install required packages\n",
        "!pip install --quiet google-cloud-aiplatform pandas matplotlib seaborn nltk tqdm python-dotenv scikit-learn\n",
        "!pip install --quiet langchain langchain-google-vertexai\n",
        "\n",
        "\n",
        "# Data processing\n",
        "\n",
        "# Text processing\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Progress tracking\n",
        "\n",
        "# Google Cloud and Vertex AI\n",
        "\n",
        "# LangChain integration\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, \n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ## 2. Google Cloud Setup and Authentication\n",
        "\n",
        "def setup_google_cloud(project_id: str, location: str = \"us-central1\") -> None:\n",
        "    \"\"\"\n",
        "    Initialize Google Cloud and Vertex AI with proper authentication.\n",
        "    \n",
        "    Args:\n",
        "        project_id: Google Cloud Project ID\n",
        "        location: Google Cloud region (default: us-central1)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize Vertex AI\n",
        "        vertexai.init(project=project_id, location=location)\n",
        "        aiplatform.init(project=project_id, location=location)\n",
        "        \n",
        "        logger.info(f\"Successfully initialized Vertex AI with project: {project_id} in {location}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to initialize Google Cloud: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# ## 3. Data Ingestion\n",
        "\n",
        "def load_data(data_path: str, source_type: str = \"local\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load data from a specified location (local or GCS).\n",
        "    \n",
        "    Args:\n",
        "        data_path: Path to the CSV file\n",
        "        source_type: 'local' or 'gcs'\n",
        "    \n",
        "    Returns:\n",
        "        Pandas DataFrame containing the loaded data\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if source_type.lower() == \"local\":\n",
        "            df = pd.read_csv(data_path)\n",
        "        elif source_type.lower() == \"gcs\":\n",
        "            # For GCS paths, use pandas' ability to read directly from GCS with gs:// prefix\n",
        "            df = pd.read_csv(f\"gs://{data_path.lstrip('gs://')}\")\n",
        "        else:\n",
        "            raise ValueError(\"Source type must be 'local' or 'gcs'\")\n",
        "        \n",
        "        logger.info(f\"Successfully loaded data from {data_path}: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading data: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def validate_data(df: pd.DataFrame, text_column: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Validate and perform basic cleaning on the dataset.\n",
        "    \n",
        "    Args:\n",
        "        df: Input DataFrame\n",
        "        text_column: Name of the column containing text data\n",
        "    \n",
        "    Returns:\n",
        "        Validated and cleaned DataFrame\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting data validation...\")\n",
        "    \n",
        "    # Check if text column exists\n",
        "    if text_column not in df.columns:\n",
        "        raise ValueError(f\"Text column '{text_column}' not found in data\")\n",
        "    \n",
        "    # Check for missing values in text column\n",
        "    missing_count = df[text_column].isna().sum()\n",
        "    if missing_count > 0:\n",
        "        logger.warning(f\"Found {missing_count} missing values in text column\")\n",
        "        df = df.dropna(subset=[text_column])\n",
        "        logger.info(f\"Dropped {missing_count} rows with missing text values\")\n",
        "    \n",
        "    # Remove empty texts\n",
        "    empty_count = (df[text_column].str.strip() == \"\").sum()\n",
        "    if empty_count > 0:\n",
        "        logger.warning(f\"Found {empty_count} empty text fields\")\n",
        "        df = df[df[text_column].str.strip() != \"\"]\n",
        "        logger.info(f\"Dropped {empty_count} rows with empty text\")\n",
        "    \n",
        "    # Basic statistics\n",
        "    df['text_length'] = df[text_column].str.len()\n",
        "    logger.info(f\"Text length statistics: Min={df['text_length'].min()}, \"\n",
        "                f\"Max={df['text_length'].max()}, \"\n",
        "                f\"Avg={df['text_length'].mean():.1f}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# ## 4. Text Preprocessing\n",
        "\n",
        "def preprocess_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Clean and normalize text.\n",
        "    \n",
        "    Args:\n",
        "        text: Raw input text\n",
        "    \n",
        "    Returns:\n",
        "        Preprocessed text\n",
        "    \"\"\"\n",
        "    # Convert to string just in case\n",
        "    text = str(text)\n",
        "    \n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "    \n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 100) -> List[str]:\n",
        "    \"\"\"\n",
        "    Split text into chunks of appropriate size for LLM processing.\n",
        "    \n",
        "    Args:\n",
        "        text: Text to split\n",
        "        chunk_size: Maximum number of characters per chunk\n",
        "        overlap: Number of characters to overlap between chunks\n",
        "    \n",
        "    Returns:\n",
        "        List of text chunks\n",
        "    \"\"\"\n",
        "    if not text or len(text) <= chunk_size:\n",
        "        return [text] if text else []\n",
        "    \n",
        "    # Use sentence tokenization for more natural splits\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        sentence_len = len(sentence)\n",
        "        \n",
        "        # If adding this sentence exceeds chunk size, save current chunk and start new one\n",
        "        if current_length + sentence_len > chunk_size and current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            \n",
        "            # Keep some sentences for overlap\n",
        "            overlap_size = 0\n",
        "            overlap_sentences = []\n",
        "            \n",
        "            # Add sentences from the end until we reach desired overlap\n",
        "            for s in reversed(current_chunk):\n",
        "                if overlap_size + len(s) <= overlap:\n",
        "                    overlap_sentences.insert(0, s)\n",
        "                    overlap_size += len(s) + 1  # +1 for space\n",
        "                else:\n",
        "                    break\n",
        "            \n",
        "            current_chunk = overlap_sentences\n",
        "            current_length = overlap_size\n",
        "        \n",
        "        current_chunk.append(sentence)\n",
        "        current_length += sentence_len + 1  # +1 for space\n",
        "    \n",
        "    # Add the last chunk if it's not empty\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "def generate_embeddings(texts: List[str], project_id: str, location: str = \"us-central1\") -> List[List[float]]:\n",
        "    \"\"\"\n",
        "    Generate embeddings for text chunks using Vertex AI embeddings.\n",
        "    \n",
        "    Args:\n",
        "        texts: List of text chunks\n",
        "        project_id: Google Cloud Project ID\n",
        "        location: Google Cloud region\n",
        "        \n",
        "    Returns:\n",
        "        List of embedding vectors\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize embedding model\n",
        "        embedding_model = VertexAIEmbeddings(\n",
        "            model_name=\"textembedding-gecko@latest\",\n",
        "            project=project_id,\n",
        "            location=location\n",
        "        )\n",
        "        \n",
        "        # Generate embeddings (batch processing)\n",
        "        logger.info(f\"Generating embeddings for {len(texts)} text chunks\")\n",
        "        embeddings = embedding_model.embed_documents(texts)\n",
        "        \n",
        "        logger.info(f\"Successfully generated embeddings of dimension {len(embeddings[0])}\")\n",
        "        return embeddings\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error generating embeddings: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# ## 5. Vertex AI and Llama 3.3 Integration\n",
        "\n",
        "def initialize_llama_model(model_name: str = \"llama-3-8b-text-001\") -> Any:\n",
        "    \"\"\"\n",
        "    Initialize the Llama 3.3 model on Vertex AI.\n",
        "    \n",
        "    Args:\n",
        "        model_name: The specific Llama model to use\n",
        "        \n",
        "    Returns:\n",
        "        Initialized model object\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize the Llama model\n",
        "        model = TextGenerationModel.from_pretrained(model_name)\n",
        "        logger.info(f\"Successfully initialized Llama model: {model_name}\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to initialize Llama model: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def initialize_generative_model(model_name: str = \"gemini-1.5-pro\") -> Any:\n",
        "    \"\"\"\n",
        "    Initialize a multimodal generative model (alternative to Llama).\n",
        "    \n",
        "    Args:\n",
        "        model_name: The specific model to use\n",
        "        \n",
        "    Returns:\n",
        "        Initialized model object\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize model\n",
        "        model = GenerativeModel(model_name)\n",
        "        logger.info(f\"Successfully initialized generative model: {model_name}\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to initialize generative model: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# ## 6. LLM Operations\n",
        "\n",
        "def process_chunk_with_llm(\n",
        "    model: Any,\n",
        "    text_chunk: str,\n",
        "    prompt_template: str,\n",
        "    temperature: float = 0.2,\n",
        "    max_output_tokens: int = 1024,\n",
        "    top_p: float = 0.95,\n",
        "    top_k: int = 40\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Send a text chunk to the LLM and get the response.\n",
        "    \n",
        "    Args:\n",
        "        model: Initialized LLM model\n",
        "        text_chunk: Text to process\n",
        "        prompt_template: Template with {text} placeholder\n",
        "        temperature: Controls randomness (lower is more deterministic)\n",
        "        max_output_tokens: Maximum tokens to generate in response\n",
        "        top_p: Nucleus sampling parameter\n",
        "        top_k: Top-k sampling parameter\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary with LLM response and metadata\n",
        "    \"\"\"\n",
        "    # Prepare the prompt by inserting the text into the template\n",
        "    prompt = prompt_template.format(text=text_chunk)\n",
        "    \n",
        "    try:\n",
        "        # Get response from the model\n",
        "        start_time = time.time()\n",
        "        \n",
        "        response = model.predict(\n",
        "            prompt,\n",
        "            temperature=temperature,\n",
        "            max_output_tokens=max_output_tokens,\n",
        "            top_p=top_p,\n",
        "            top_k=top_k\n",
        "        )\n",
        "        \n",
        "        processing_time = time.time() - start_time\n",
        "        \n",
        "        return {\n",
        "            \"input_text\": text_chunk,\n",
        "            \"response_text\": response.text,\n",
        "            \"processing_time\": processing_time,\n",
        "            \"prompt\": prompt,\n",
        "            \"success\": True,\n",
        "            \"timestamp\": time.time()\n",
        "        }\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing chunk with LLM: {str(e)}\")\n",
        "        return {\n",
        "            \"input_text\": text_chunk,\n",
        "            \"response_text\": \"\",\n",
        "            \"error\": str(e),\n",
        "            \"success\": False,\n",
        "            \"timestamp\": time.time()\n",
        "        }\n",
        "\n",
        "def batch_process_with_llm(\n",
        "    model: Any,\n",
        "    text_chunks: List[str],\n",
        "    prompt_template: str,\n",
        "    batch_size: int = 5,\n",
        "    max_retries: int = 3,\n",
        "    retry_delay: int = 2,\n",
        "    **model_params\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Process multiple text chunks with LLM in batches with retry logic.\n",
        "    \n",
        "    Args:\n",
        "        model: Initialized LLM model\n",
        "        text_chunks: List of text chunks to process\n",
        "        prompt_template: Template for prompt with {text} placeholder\n",
        "        batch_size: Number of parallel requests\n",
        "        max_retries: Maximum number of retries for failed requests\n",
        "        retry_delay: Seconds to wait between retries\n",
        "        model_params: Parameters for the LLM model\n",
        "        \n",
        "    Returns:\n",
        "        List of response dictionaries\n",
        "    \"\"\"\n",
        "    all_results = []\n",
        "    \n",
        "    for i in tqdm(range(0, len(text_chunks), batch_size), desc=\"Processing batches\"):\n",
        "        batch = text_chunks[i:i + batch_size]\n",
        "        batch_results = []\n",
        "        \n",
        "        for chunk in batch:\n",
        "            # Process with retry logic\n",
        "            for attempt in range(max_retries + 1):\n",
        "                result = process_chunk_with_llm(model, chunk, prompt_template, **model_params)\n",
        "                \n",
        "                if result[\"success\"]:\n",
        "                    batch_results.append(result)\n",
        "                    break\n",
        "                elif attempt < max_retries:\n",
        "                    logger.warning(f\"Retry {attempt + 1}/{max_retries} after error: {result.get('error', 'Unknown error')}\")\n",
        "                    time.sleep(retry_delay)\n",
        "                else:\n",
        "                    logger.error(f\"Failed to process chunk after {max_retries} retries: {chunk[:100]}...\")\n",
        "                    batch_results.append(result)\n",
        "        \n",
        "        all_results.extend(batch_results)\n",
        "        \n",
        "        # Add a small delay between batches to avoid rate limiting\n",
        "        if i + batch_size < len(text_chunks):\n",
        "            time.sleep(1)\n",
        "    \n",
        "    # Log processing statistics\n",
        "    success_count = sum(1 for r in all_results if r[\"success\"])\n",
        "    logger.info(f\"Processing complete: {success_count}/{len(all_results)} chunks successful\")\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "# ## 7. Results and Visualization\n",
        "\n",
        "def analyze_results(results: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Analyze the results from LLM processing.\n",
        "    \n",
        "    Args:\n",
        "        results: List of LLM response dictionaries\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary with analysis metrics\n",
        "    \"\"\"\n",
        "    # Filter successful results\n",
        "    successful = [r for r in results if r[\"success\"]]\n",
        "    \n",
        "    if not successful:\n",
        "        logger.warning(\"No successful results to analyze\")\n",
        "        return {\"success_rate\": 0}\n",
        "    \n",
        "    # Extract basic metrics\n",
        "    response_lengths = [len(r[\"response_text\"]) for r in successful]\n",
        "    processing_times = [r[\"processing_time\"] for r in successful]\n",
        "    \n",
        "    analysis = {\n",
        "        \"success_rate\": len(successful) / len(results) if results else 0,\n",
        "        \"total_chunks\": len(results),\n",
        "        \"successful_chunks\": len(successful),\n",
        "        \"avg_response_length\": sum(response_lengths) / len(successful) if successful else 0,\n",
        "        \"min_response_length\": min(response_lengths) if response_lengths else 0,\n",
        "        \"max_response_length\": max(response_lengths) if response_lengths else 0,\n",
        "        \"avg_processing_time\": sum(processing_times) / len(successful) if successful else 0,\n",
        "        \"total_processing_time\": sum(processing_times) if successful else 0,\n",
        "    }\n",
        "    \n",
        "    return analysis\n",
        "\n",
        "def visualize_results(results: List[Dict[str, Any]], analysis: Dict[str, Any]):\n",
        "    \"\"\"\n",
        "    Visualize the results from LLM processing.\n",
        "    \n",
        "    Args:\n",
        "        results: List of LLM response dictionaries\n",
        "        analysis: Analysis metrics from analyze_results function\n",
        "    \"\"\"\n",
        "    # Filter successful results\n",
        "    successful = [r for r in results if r[\"success\"]]\n",
        "    \n",
        "    if not successful:\n",
        "        logger.warning(\"No successful results to visualize\")\n",
        "        return\n",
        "    \n",
        "    # Set up the plotting area\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    \n",
        "    # 1. Success rate pie chart\n",
        "    plt.subplot(2, 2, 1)\n",
        "    labels = ['Success', 'Failed']\n",
        "    sizes = [analysis['successful_chunks'], analysis['total_chunks'] - analysis['successful_chunks']]\n",
        "    colors = ['#66b3ff', '#ff9999']\n",
        "    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "    plt.axis('equal')\n",
        "    plt.title('Processing Success Rate')\n",
        "    \n",
        "    # 2. Response length distribution\n",
        "    plt.subplot(2, 2, 2)\n",
        "    response_lengths = [len(r[\"response_text\"]) for r in successful]\n",
        "    sns.histplot(response_lengths, kde=True, bins=20)\n",
        "    plt.title('Distribution of Response Lengths')\n",
        "    plt.xlabel('Response Length (characters)')\n",
        "    plt.ylabel('Frequency')\n",
        "    \n",
        "    # 3. Processing time distribution\n",
        "    plt.subplot(2, 2, 3)\n",
        "    processing_times = [r[\"processing_time\"] for r in successful]\n",
        "    sns.histplot(processing_times, kde=True, bins=20)\n",
        "    plt.title('Distribution of Processing Times')\n",
        "    plt.xlabel('Processing Time (seconds)')\n",
        "    plt.ylabel('Frequency')\n",
        "    \n",
        "    # 4. Input vs Output length scatter plot\n",
        "    plt.subplot(2, 2, 4)\n",
        "    input_lengths = [len(r[\"input_text\"]) for r in successful]\n",
        "    output_lengths = [len(r[\"response_text\"]) for r in successful]\n",
        "    plt.scatter(input_lengths, output_lengths, alpha=0.5)\n",
        "    plt.title('Input Length vs. Output Length')\n",
        "    plt.xlabel('Input Length (characters)')\n",
        "    plt.ylabel('Output Length (characters)')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('llm_results_visualization.png')\n",
        "    plt.show()\n",
        "    \n",
        "    logger.info(\"Visualizations generated and saved to 'llm_results_visualization.png'\")\n",
        "\n",
        "def save_results(results: List[Dict[str, Any]], output_path: str = \"llm_results.json\"):\n",
        "    \"\"\"\n",
        "    Save processing results to file.\n",
        "    \n",
        "    Args:\n",
        "        results: List of LLM response dictionaries\n",
        "        output_path: File path for saving results\n",
        "    \"\"\"\n",
        "    # Create a serializable version of the results\n",
        "    serializable_results = []\n",
        "    \n",
        "    for result in results:\n",
        "        # Create a clean copy without any non-serializable elements\n",
        "        clean_result = {\n",
        "            \"input_text\": result[\"input_text\"][:100] + \"...\" if len(result[\"input_text\"]) > 100 else result[\"input_text\"],\n",
        "            \"response_text\": result[\"response_text\"],\n",
        "            \"success\": result[\"success\"],\n",
        "            \"processing_time\": result.get(\"processing_time\", None),\n",
        "            \"timestamp\": result.get(\"timestamp\", None)\n",
        "        }\n",
        "        \n",
        "        if \"error\" in result:\n",
        "            clean_result[\"error\"] = str(result[\"error\"])\n",
        "            \n",
        "        serializable_results.append(clean_result)\n",
        "    \n",
        "    # Save to file\n",
        "    try:\n",
        "        with open(output_path, 'w') as f:\n",
        "            json.dump(serializable_results, f, indent=2)\n",
        "        logger.info(f\"Results saved to {output_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error saving results: {str(e)}\")\n",
        "\n",
        "# ## 8. Example Pipeline Execution Function\n",
        "\n",
        "def run_llm_processing_pipeline(\n",
        "    data_path: str,\n",
        "    text_column: str,\n",
        "    project_id: str,\n",
        "    prompt_template: str,\n",
        "    location: str = \"us-central1\",\n",
        "    source_type: str = \"local\",\n",
        "    chunk_size: int = 1000,\n",
        "    chunk_overlap: int = 100,\n",
        "    model_name: str = \"llama-3-8b-text-001\",\n",
        "    batch_size: int = 3,\n",
        "    sample_size: Optional[int] = None\n",
        ") -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Execute the full LLM processing pipeline.\n",
        "    \n",
        "    Args:\n",
        "        data_path: Path to the data file\n",
        "        text_column: Name of the column containing text\n",
        "        project_id: Google Cloud Project ID\n",
        "        prompt_template: Template for LLM prompts with {text} placeholder\n",
        "        location: Google Cloud region\n",
        "        source_type: Data source type ('local' or 'gcs')\n",
        "        chunk_size: Maximum size of text chunks\n",
        "        chunk_overlap: Overlap between chunks\n",
        "        model_name: Vertex AI model name\n",
        "        batch_size: Batch size for LLM processing\n",
        "        sample_size: Optional limit on number of rows to process\n",
        "        \n",
        "    Returns:\n",
        "        Tuple of (results list, analysis dictionary)\n",
        "    \"\"\"\n",
        "    # 1. Setup\n",
        "    setup_google_cloud(project_id, location)\n",
        "    \n",
        "    # 2. Data ingestion\n",
        "    df = load_data(data_path, source_type)\n",
        "    df = validate_data(df, text_column)\n",
        "    \n",
        "    # Optional sampling for testing\n",
        "    if sample_size and sample_size < len(df):\n",
        "        df = df.sample(sample_size, random_state=42)\n",
        "        logger.info(f\"Using sample of {sample_size} records for processing\")\n",
        "    \n",
        "    # 3. Text preprocessing\n",
        "    logger.info(\"Preprocessing text data...\")\n",
        "    df['processed_text'] = df[text_column].apply(preprocess_text)\n",
        "    \n",
        "    # 4. Text chunking\n",
        "    logger.info(f\"Chunking text with chunk_size={chunk_size}, overlap={chunk_overlap}\")\n",
        "    all_chunks = []\n",
        "    for text in tqdm(df['processed_text'], desc=\"Chunking texts\"):\n",
        "        chunks = chunk_text(text, chunk_size=chunk_size, overlap=chunk_overlap)\n",
        "        all_chunks.extend(chunks)\n",
        "    \n",
        "    logger.info(f\"Created {len(all_chunks)} chunks from {len(df)} documents\")\n",
        "    \n",
        "    # 5. Initialize Llama model\n",
        "    model = initialize_llama_model(model_name)\n",
        "    \n",
        "    # 6. Process chunks with LLM\n",
        "    logger.info(f\"Processing {len(all_chunks)} chunks with LLM in batches of {batch_size}\")\n",
        "    results = batch_process_with_llm(\n",
        "        model=model,\n",
        "        text_chunks=all_chunks,\n",
        "        prompt_template=prompt_template,\n",
        "        batch_size=batch_size,\n",
        "        temperature=0.2,\n",
        "        max_output_tokens=1024\n",
        "    )\n",
        "    \n",
        "    # 7. Analyze and visualize results\n",
        "    analysis = analyze_results(results)\n",
        "    logger.info(f\"Analysis complete: {analysis['success_rate']:.1%} success rate\")\n",
        "    \n",
        "    visualize_results(results, analysis)\n",
        "    save_results(results, \"llm_results.json\")\n",
        "    \n",
        "    return results, analysis\n",
        "\n",
        "# Example usage of the pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    # Demo configuration\n",
        "    PROJECT_ID = \"your-gcp-project-id\"  # Replace with actual project ID\n",
        "    PROMPT_TEMPLATE = \"\"\"\n",
        "    Please analyze the following text and extract the main topics, key entities, and sentiment:\n",
        "    \n",
        "    {text}\n",
        "    \n",
        "    Response format:\n",
        "    - Main topics: [List the key topics]\n",
        "    - Key entities: [List important entities]\n",
        "    - Sentiment: [Positive/Negative/Neutral with brief explanation]\n",
        "    \"\"\"\n",
        "    \n",
        "    # Run pipeline with small sample for demo\n",
        "    results, analysis = run_llm_processing_pipeline(\n",
        "        data_path=\"sample_data.csv\",        # Replace with actual data path\n",
        "        text_column=\"content\",              # Replace with actual column name\n",
        "        project_id=PROJECT_ID,\n",
        "        prompt_template=PROMPT_TEMPLATE,\n",
        "        sample_size=10                      # Small sample for demonstration\n",
        "    )\n",
        "    \n",
        "    print(f\"Pipeline execution complete. Processed {len(results)} text chunks.\")\n",
        "    print(f\"Success rate: {analysis['success_rate']:.1%}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPLRTeUWE2cZtr7H1An5Ds3",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
